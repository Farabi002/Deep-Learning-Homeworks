{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense,Activation, Dropout\nfrom tensorflow.keras.models import Model\n\n# Define the input shape\ninput_shape = (224, 224, 3)\n\n# Define the input tensor\ninputs = Input(shape=input_shape)\n\n# Add a Conv2D layer with 64 filters, a 3x3 kernel, and 'relu' activation\nx = Conv2D(64, (3, 3), activation='relu')(inputs)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n\n# Add another Conv2D layer with 64 filters, a 3x3 kernel, and 'relu' activation\nx = Conv2D(64, (3, 3), activation='relu')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n\n# Add another Conv2D layer with 96 filters, a 3x3 kernel, and 'relu' activation\nx = Conv2D(96, (3, 3), activation='relu')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Add another Conv2D layer with 96 filters, a 3x3 kernel, and 'relu' activation\nx = Conv2D(96, (3, 3), activation='relu')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n\n# Flatten the feature maps\nx = Flatten()(x)\n\n# Add a fully connected Dense layer with 128 units and 'relu' activation\nx = Dense(256, activation='relu')(x)\nx= Dropout(0.6)(x)\n\nx = Dense(128, activation='relu')(x)\nx= Dropout(0.3)(x)\n\nx = Dense(64, activation='relu')(x)\nx= Dropout(0.15)(x)\n\nx = Dense(32, activation='relu')(x)\nx= Dropout(0.075)(x)\n\nx = Dense(16, activation='relu')(x)\nx= Dropout(0.0325)(x)\n\n\n# Add the output layer with the desired number of units and activation function\noutputs = Dense(8, activation='softmax')(x)\n\n\n","metadata":{},"execution_count":null,"outputs":[]}]}