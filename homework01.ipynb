{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5526880,"sourceType":"datasetVersion","datasetId":3186287}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Optimize the model to get at least 70% accuray .you may play with\n\n1. Dynamic learning rate\n2. Save model\n3. Early stopping\n4. Drop-Out**\nfor getting higher accuracy","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport glob\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## import matplotlib.pyplot as plt\nimport glob\nimport librosa\nimport numpy as np\nimport cv2 \nbleached_corals = glob.glob('/kaggle/input/healthy-and-bleached-corals-image-classification/bleached_corals/*.jpg')\nhealthy_corals = glob.glob('/kaggle/input/healthy-and-bleached-corals-image-classification/healthy_corals/*.jpg')\n\ndata = []\nlabels = []\n\nimage_size=(32,32)\n\nfor i in bleached_corals:   \n    image = cv2.imread(i,cv2.IMREAD_GRAYSCALE)\n    # Resize the image\n    image = cv2.resize(image, image_size)\n\n    data.append(image)\n    labels.append(0)\n\nfor i in healthy_corals:   \n    image = cv2.imread(i,cv2.IMREAD_GRAYSCALE)\n    # Resize the image\n    image = cv2.resize(image, image_size)\n\n    data.append(image)\n    labels.append(1)\n    \n    \nX=np.array(data)\ny=np.array(labels)\nX.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-04T07:55:36.067887Z","iopub.execute_input":"2023-12-04T07:55:36.068279Z","iopub.status.idle":"2023-12-04T07:55:44.601500Z","shell.execute_reply.started":"2023-12-04T07:55:36.068250Z","shell.execute_reply":"2023-12-04T07:55:44.600391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Perform train-validation split\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\nX_train = X_train / 255.0\nX_val = X_val / 255.0\nX_test = X_test / 255.0\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T07:56:08.794534Z","iopub.execute_input":"2023-12-04T07:56:08.795116Z","iopub.status.idle":"2023-12-04T07:56:08.818278Z","shell.execute_reply.started":"2023-12-04T07:56:08.795063Z","shell.execute_reply":"2023-12-04T07:56:08.816882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n# Build the MLP model\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(input_shape=(32,32)),\n    \n    tf.keras.layers.Dense(units=256, activation='relu'),\n    tf.keras.layers.Dense(units=128, activation='relu'),\n    tf.keras.layers.Dense(units=64, activation='relu'),\n    tf.keras.layers.Dense(units=1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Define the learning rate reduction callback\nlr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_accuracy',   # Monitor validation loss for learning rate reduction\n    factor=0.1,            # Reduce learning rate by a factor of 0.1\n    patience=4,            # Number of epochs with no improvement after which learning rate will be reduced\n    min_lr=2e-7           # Minimum learning rate\n)\n\n\n\n# Define the model checkpoint callback to save the best model\ncheckpoint_callback = ModelCheckpoint('best_model2', monitor='val_accuracy', save_best_only=True, mode= 'max')\n\n# Define the early stopping callback to stop training if validation loss does not improve\nearly_stopping_callback = EarlyStopping(monitor='val_accuracy', patience=5)\n\n# Train the model with the learning rate reduction callback\nmodel.fit(X_train, y_train,validation_data=(X_val,y_val),epochs=20, batch_size=8,callbacks=[lr_callback,checkpoint_callback,early_stopping_callback])\n\n# Evaluate the model\nbest_model = tf.keras.models.load_model('best_model2')\nloss, accuracy = best_model.evaluate(X_test, y_test)\n\nprint(loss, accuracy )\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T08:46:58.843496Z","iopub.execute_input":"2023-12-04T08:46:58.844275Z","iopub.status.idle":"2023-12-04T08:47:09.110820Z","shell.execute_reply.started":"2023-12-04T08:46:58.844233Z","shell.execute_reply":"2023-12-04T08:47:09.110007Z"},"trusted":true},"execution_count":null,"outputs":[]}]}